---
title: "Exploring timing data"
author: "Kirill MÃ¼ller"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Exploring timing data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r}
library(wrswoR)
library(ggplot2)
library(dplyr)
library(tidyr)
```

# Run time tests

- Input sizes: `r unique(timings$n)`
- Probability distributions: `r unique(timings$prob)`
- Algorithms: `r unique(timings$expr)`
- Total measurements: `r nrow(timings)`

Clearly worse performance of R implementation for large inputs.

```{r fig.width=7, fig.height=5}
timings %>%
  filter(prob == "uniform") %>%
  ggplot(aes(x=factor(n), y=time * 1e-9, color=expr)) +
  geom_boxplot() +
  scale_y_log10()
```

Compare median and worst performance (for uniform distribution).

```{r fig.width=7, fig.height=5}
timings %>%
  filter(prob == "uniform") %>%
  group_by(n, expr) %>%
  summarize(median = median(time), max = max(time)) %>%
  ungroup %>%
  gather(metric, time, median, max) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~metric, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

Worst-best comparison: best for R, worst for others (but per probability distribution).

```{r fig.width=7, fig.height=5}
timings %>%
  filter(n >= 100 & n < 10000) %>%
  group_by(n, expr, prob) %>%
  summarize(minmax = if (expr[[1]] == "R") min(time) else max(time)) %>%
  ungroup %>%
  gather(metric, time, minmax) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

Same as above, now taking 5% and 95% quantiles.

```{r fig.width=7, fig.height=5}
timings %>%
  filter(n >= 100 & n <= 10000) %>%
  group_by(n, expr, prob) %>%
  summarize(minmax = if (expr[[1]] == "R") min(time) else max(time)) %>%
  ungroup %>%
  gather(metric, time, minmax) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

## More detailed data for break-even analysis

Min-max very sensitive to garbage collection
(see also https://radfordneal.wordpress.com/2014/02/02/inaccurate-results-from-microbenchmark/):

```{r fig.width=7, fig.height=5}
break_even %>%
  group_by(n, expr, prob) %>%
  summarize(minmax = if (expr[[1]] == "R") min(time) else max(time)) %>%
  ungroup %>%
  gather(metric, time, minmax) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

Better take a quantile (say, 95th):

```{r fig.width=7, fig.height=5}
break_even %>%
  group_by(n, expr, prob) %>%
  summarize(minmax = if (expr[[1]] == "R") quantile(time, 0.05) else quantile(time, 0.95)) %>%
  ungroup %>%
  gather(metric, time, minmax) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

Even better, avoid allocation of R vectors in Rcpp code.


## Compare distributions

Box plots for relative differences beween non-uniform and uniform sampling.

```{r fig.width=7, fig.height=5}
timings %>%
  arrange(prob, n, expr, time) %>%
  group_by(prob, n, expr) %>%
  mutate(id = seq_along(time)) %>%
  ungroup %>%
  spread(prob, time) %>%
  gather_("diff_to", "other_time", unique(timings$prob) %>% setdiff("uniform")) %>%
  mutate(time_diff = (other_time - uniform) / uniform, uniform = NULL, other_time = NULL) %>%
  ggplot(aes(x=factor(n), y=time_diff, color=expr)) +
  facet_wrap(~diff_to) +
  scale_y_continuous(limits = c(-1, 1)) +
  geom_boxplot()
```



# Comparison of results
