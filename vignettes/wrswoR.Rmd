---
author:
  - name: Kirill Müller
    affiliation: IVT, ETH Zurich
    address: >
      Stefano-Franscini-Platz 9
      8093 Zürich
    email: kirill.mueller@ivt.baug.ethz.ch
    url: http://www.ivt.ethz.ch
title:
  formatted: "Accelerating weighted random sampling without replacement"
  # If you use tex in the formatted title, also supply version without
  plain:     "Accelerating weighted random sampling without replacement"
  # For running headers, if needed
  short:     "Accelerating weighted random sampling without replacement"
abstract: >
  Random sampling from discrete populations is a central part of Monte-Carlo
  simulations. (What other applications?)
  This article briefly introduces weighted and unweighted sampling with and
  without replacement.
  The case of weighted sampling without replacement appears to be most difficult
  to implement, which might be one reason why the \proglang{R} implementation
  performs slowly for large input.
  Four alternative implementations for the case of weighted
  sampling without replacement are presented and validated.
keywords:
  # at least one keyword must be supplied
  formatted: [Weighted sampling, performance, meta-analysis, "\\proglang{R}"]
  plain:     [Weighted sampling, performance, meta-analysis, R]
preamble: >
  \usepackage{amsmath}
  \usepackage[USenglish]{babel}
  \usepackage{algorithm}
  \usepackage{algorithmic}
  \usepackage[draft]{fixme}
  \usepackage[capitalize]{cleveref}
  \usepackage{ragged2e}
  \usepackage{mathtools}
  \usepackage{siunitx}
output: rticles::jss_article
bibliography: [ my.bib, knitcitations.bib ]
vignette: >
  %\VignetteIndexEntry{Accelerating weighted random sampling without replacement}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r echo=FALSE, message=FALSE}
library(wrswoR)
import::from(tidyr, spread, gather, gather_, .library = .libPaths())
import::from(plyr, ldply, .library = .libPaths())
import::from(dplyr, filter, mutate, select, group_by, summarize, ungroup, tbl_df,
             rename, arrange, mutate_each, funs, .library = .libPaths())
import::from(cluster, daisy, .library = .libPaths())
import::from(magrittr, "%>%", .library = .libPaths())
library(ggplot2)
set.seed(20150710L)
knitr::opts_chunk$set(cache=TRUE)
knitcitations::cite_options("pandoc")

knit_print.function <- function(x, options) {
  dput(x)
}
```

# Introduction

\fxwarning{(Everywhere) Refer to weights and not probabilities}

Random sampling is one of the basic primitives in statistical computing.
\fxwarning{Used where?}

This paper focuses on a specific variant: sampling without replacement from a finite population
with non-uniform probability distribution.
\fxwarning{Application?}

First, different techniques for sampling from discrete populations are reviewed.
Several implementations for sampling without replacement are discussed,
this includes evaluation of runtime performance and correctness.
The paper concludes with suggestions for incorporating the findings into base
\proglang{R}.


# Sampling from discrete populations

\fxwarning{Longer introduction}
\fxwarning{Different from Tillé's sampling}
We use \cref{alg:sample} as a definition of sampling from discrete populations
with or without replacement from arbitrary (including uniform) probability distributions.
From this definition, the following can be observed:

\begin{algorithm}
  \caption{$\text{sample}(n, s, \text{replace}, p_i)$}
  \label{alg:sample}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $\text{replace}$: \TRUE{} to request sampling with replacement
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\} $ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \IF{$s = 0$}
      \RETURN vector of length 0
    \ENDIF
    \STATE Randomly select $k$ so that $\mathrm{P}(k=i) = \frac{p_k}{\sum_{j}p_j}$
      for all $i$ \label{alg:sample:norm}
    \IF{not replace}\label{alg:sample:if}
      \STATE $n \leftarrow n - 1$
      \STATE remove item $k$ from $p_i$\label{alg:sample:remove}
    \ENDIF\label{alg:sample:if-end}
    \RETURN $k \oplus \text{sample}(n, s - 1, \text{replace}, p_i)$
  \end{algorithmic}
\end{algorithm}

- Sampling with replacement appears to be a simpler problem than sampling without
  replacement, as the lines \ref{alg:sample:if} and
  \ref{alg:sample:if-end} in \cref{alg:sample} are not required.
- If all probabilities $p_i$ are equal, the selection probability $\mathrm{P}(i=k)$
  of the sampled items in line \ref{alg:sample:norm} always equals $\frac{1}{n}$
  and does not have to be computed explicitly.

In this framework, sampling without replacement with non-uniform probabilities
seems to be the hardest problem.
This also holds for the more specialized algorithms that implement
each case.


## Sampling with replacement

The *with replacement* case corresponds to repeated selection of $k$ from *the same*
discrete probability distribution.
The uniform case can be implemented easily by transforming the
output of a random number generator
that returns uniformly distributed floating-point numbers in $[0, 1)$.
(Implementing such a random number generator is nontrivial in itself
but outside the scope of this paper.)

More work is needed in the non-uniform case:
Here, Walker's alias method [@Walker__1977], which is also used in \proglang{R},
is an option.
Assuming w.l.o.g.\ $\sum_j p_j = n$,
it is possible to construct a subdivision
$(l_i, r_i, s_i)$ with $i, l_i, r_i \in \{1,\ldots,n\}$
and $0 < s_i \leq 1$ so that
$$p_i = \sum_{j:l_j=i}{s_j} + \sum_{j:r_j=i}{(1-s_j)}.$$
Sampling an item requires sampling from $\{1,\ldots,n\}$ (to choose $i$) 
and then sampling from $\left[0, 1\right)$ (to choose $l_i$ or $r_i$):
If the random number is less than $s_i$, item $l_i$ is chosen,
otherwise item $r_i$.
(Figuratively, the probability mass given by $p_i$
is distributed over $n$ "boxes" so that the space in each box $i$
is assigned to at most two items
$l_i$ and $r_i$.
The share occupied by item $l_i$ in box $i$ is given by $s_i$.
Some items may be distributed over several boxes.
Sampling an item means selecting a box and choosing between the two items in this box.)

The preprocessing time of $O(n)$ required for Walker's alias method
(in a modification suggested by @Vose_IEEETrans.Softw.Eng._1991)
is also the lower bound for arbitrary probability distributions.
It is easy to see that the entire probability vector must be read at least once.
Hence, for nonuniform probabilities,
the run time is at least $O(n + s)$, and the input size $n$ will dominate
unless $s \gg n$.
More recently, @Marsaglia_J.Stat.Softw._2004 have suggested a table-based method
that seems to perform much faster in practice but expresses the probabilities
as rationals with a fixed base and is therefore not usable directly
for distributions with a large range.
@Shmerling_StatisticsProbabilityLetters_2013 presents a comprehesive review
and suggests a general method suitable even for quasi-infinite ranges.


## Sampling without replacement

In the *without replacement* case, each selected item is removed from the
collection of candidate items.
Again, the uniform case is much simpler.
An array of size $n$, initialized with the natural sequence,
can be used for storing the candidate items.
The selection of the item corresponds to choosing an index at random in this
array.
Removal of an item with known index can be done in $O(1)$ time
by simply replacing it with the last item in the array
and truncating the array by one.
\fxwarning{Reservoir for $s \ll n$}
For the non-uniform case,
lines \ref{alg:sample:norm} and \ref{alg:sample:remove}
in \cref{alg:sample} can be implemented with
a data structure that maintains a subdivision of an interval
into $n$ subintervals and allows lookups and updates.
Walker's alias method seems to be ill-suited for this purpose,
as each item potentially spreads over several "boxes",
which can make updates costly.
@Wong__1980 propose a data structure similar to a heap
that can be initialized in $O(n)$ time and supports
simultaneous lookup and update in $O(\log n)$ time,
we refer to the original paper for details.

# Explanation of the code

\proglang{R} offers reasonably efficient implementations for all cases except
non-uniform sampling without replacement,
the latter requires $O(n \cdot s)$ run time,
which is equivalent to $O(n^2)$ if $s = O(n)$.
This paper explores alternative approaches:
rejection sampling, one-pass sampling and reservoir sampling.
Only the first can be described formally within the framework of \cref{alg:sample},
however an actual implementation would use sampling *with* replacement as a subroutine.
The last two are based on arithmetic transformation of a probability distribution.


## Rejection sampling

In the framework of \cref{alg:sample},
rejection sampling corresponds to flagging sampled items as "invalid"
(instead of removing them) in line \ref{alg:sample:remove},
and repeating the sampling in line \ref{alg:sample:norm}
until hitting a valid item.
Note that the distribution of the result is not modified if invalid items
are purged occasionally.

Therefore, sampling without replacement can be emulated by
repeated sampling with replacement, as shown in \cref{alg:sample-rej}.
\fxwarning{Reference? Check Yates (1960)}
The general idea is to sample slightly more items than necessary (with replacement),
and then to throw away the duplicate items.
If the resulting sequence of items is shorter than requested,
the result for a much smaller problem is appended.
In \cref{alg:sample-rej},
duplicate items in the result of a sampling with replacement
(line \ref{alg:sample-rej:sample})
correspond to invalid items in the rejection sampling,
and the recursive call in line \ref{alg:sample-rej:rec} corresponds to
purging the invalid items.



\begin{algorithm}
  \caption{$\text{sample.rej}(n, s, p_i)$}
  \label{alg:sample-rej}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\} $ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \STATE $k_i \leftarrow \text{unique}(\text{sample}(n, \text{expected.items}(n, s), \TRUE, p_i))$
      \label{alg:sample-rej:sample}
    \STATE $l \leftarrow \text{length}(k_i)$
    \IF{$l \geq s$}
      \RETURN the first $s$ items of $k_i$
    \ENDIF
    \STATE remove items $k_i$ from $p_i$\label{alg:sample-rej:remove}
    \RETURN $k_i \oplus \text{sample.rej}(n - l, s - l, p_i)$\label{alg:sample-rej:rec}
  \end{algorithmic}
\end{algorithm}

Here, the function expected.items() estimates
the number of items that need to be drawn with replacement,
so that the result can be expected to contain at least $s$ unique items.
(An incorrect estimate only affects the run time, not the correctness
of the algorithm.)
For a uniform distribution, it can be shown that, with
$\text{expected.items}(n, s) \geq n (H_n - H_{n-s}) = n \sum_{i=n-s+1}^n \frac{1}{i}$,
the result has $s$ unique items in expectation.
\fxwarning{Expected amortized run time? Look it up in a reference}
(With $\text{expected.items}(n, s) = 1$ everywhere,
\cref{alg:sample,alg:sample-rej} are in fact identical.)


## Reservoir sampling

A particularly interesting algorithm has been devised only recently by
@efraimidis_weighted_2006.
In the simplest version, it is sufficient to draw $n$ random numbers,
combine them arithmetically with the probability distribution $p_i$,
and perform a partial sort to find the indexes of the $s$ smallest items.
\Cref{alg:sample-rank} is a modified version of Algorithm A in the original paper
that operates on the logarithmic scale and uses multiplication
instead of exponentiation
for increased numeric stability.
\fxwarning{Describe transformation}
In line \ref{alg:sample-rank:sample}, $\text{Exp}(1)$ refers to i.i.d.\ samples
from the exponential distribution with rate 1.
\fxwarning{Logarithm of the uniform}

\begin{algorithm}
  \caption{$\text{sample.rank}(n, s, p_i)$}
  \label{alg:sample-rank}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\}$ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \STATE $r_i \leftarrow \text{Exp}(1) / p_i$ for all $i \in \{1,\ldots,n\}$
      \label{alg:sample-rank:sample}
    \RETURN the positions of the $s$ smallest elements in $r_i$\label{alg:sample-rank:rec}
  \end{algorithmic}
\end{algorithm}

The algorithm amazes with its elegance and simplicity.
Computational complexity is dominated by the partial sort
(which can be implemented in $O(n + s \log n)$,
or even in $O(n)$ for floating-point numbers
[@terdiman_radix_2000].
However, the cost of generating $n$ random variates may outweigh
the cost for sorting even for moderately large values of $s$.
To overcome this issue,
@efraimidis_weighted_2006
describe *reservoir sampling with exponential jumps* -- 
an extension where each generated random number decides how many items are skipped
until the current "least likely" item is removed from the reservoir.
Only $O(s \log \frac{n}{s})$ random numbers (in expectation) are needed with this extension,
the simple version always requires $n$ random numbers.
The exponential jumps method requires fewer updates of the reservoir
(and therefore fewer random numbers)
if the weights are arranged in descending order.
We refer to the original paper for more details, including formal proofs of correctness.


# Code

The \pkg{wrswoR} package contains implementations for the two algorithms
presented in the previous section, one \proglang{R} implementation
of rejection sampling (\cref{alg:sample-rej}, denoted by *rej*),
two implementations (\proglang{R} and \proglang{C++})
of simplified reservoir sampling (\cref{alg:sample-rank}, *rank* and *crank*)
and one \proglang{C++} implementation of reservoir sampling with exponential jumps
(*expj*).
In the package, the corresponding functions are prefixed with \code{sample_int_}.
The \pkg{Rcpp} package
`r knitcitations::citep(citation("Rcpp"))`
is used to generate the glue between \proglang{R} and \proglang{C++}.

The \proglang{R} implementations are very similar to the pseudocode:
As an example, the *rank* implementation is shown below.

```{r, echo=FALSE}
sample_int_rank
```

The function arguments correspond to those of \cref{alg:sample,alg:sample-rej,alg:sample-rank}:
\code{size} is the $s$ argument, and \code{prob} is the $p_i$ argument.

The *crank* implementation has been somewhat optimized for cache efficiency.
Due to its relative complexity, the *expj* implementation is kept very close
to the pseudocode in the original paper,
still this function also operates on the logarithmic scale for numeric stability.
The transformation works in a fashion very similar to that of
\cref{alg:sample-rank}.

All functions share the same interface.
Compared to \code{sample.int}, the base \proglang{R} function,
the \code{replace} parameter has been removed, and \code{prob} cannot be \code{NULL}.
To simplify testing the new routines against the \proglang{R} implementation,
a wrapper function \code{sample_int_R} is provided.
The subsequent section shows performance characteristics and correctness
of the new implementations compared to the base \proglang{R} version.


# Tests

This section presents run time tests for various combinations of input parameters,
attempts to provide guidance when to choose which implementation,
and discusses the correctness of the implementation.
All test results shown in this section are based on data available
in the \pkg{wrswoR.benchmark} package.


## Input parameters

The run time tests used different values for the function arguments
$n$, $s$ and $prob$.
Instead of directly specifying $s$, it is given as a fraction of $n$,
denoted by $r = \frac{s}{n}$.
The following weight distributions (used for $p_i$) were tested:

- *Uniform*
- *Linear*: Sequence from $1$ to $n$ ($\{p_i\} = \{1,\ldots n\}$, *ascending*, *descending* and *shuffled*)
- *Geometric*: Starting at $1$, the weight is multiplied with a constant $\alpha$ for each step
  ($p_{i+1} = \alpha p_i$, *ascending*, *descending*, and *shuffled*);
  the constant is chosen so that both minimal and maximal weights
  and the sum of weights is still representable as a floating-point number.
  \fxwarning{Extreme case}

```{r echo=FALSE}
ggplot_base <- list(
  theme_bw(11)
)

ALGOS <- c("R", "rej", "rank", "crank", "expj")

REMOVE_PROB <- c("linear_double", "linear_half")

timings <- 
  wrswoR.benchmark::timings %>%
  tbl_df %>%
  filter(expr %in% ALGOS) %>%
  filter(!(prob %in% REMOVE_PROB)) %>%
  mutate(expr = factor(expr, levels = ALGOS)) %>%
  mutate(prob = factor(
    prob, levels = c("uniform", "linear", "rev_linear", "shuffled_linear", "exp", "rexp"),
    labels = c("Uniform", "Linear asc.", "Linear desc.", "Linear shuffled", "Geometric asc.", "Geometric desc.")))

BASE <- 1.007
N <- max(timings$n)
```

```{r echo=FALSE}
break_even <- 
  wrswoR.benchmark::break_even %>%
  tbl_df %>%
  filter(expr %in% ALGOS) %>%
  filter(!(prob %in% REMOVE_PROB)) %>%
  mutate(expr = factor(expr, levels = ALGOS)) %>%
  mutate(prob = factor(
    prob, levels = c("uniform", "linear", "rev_linear", "shuffled_linear", "exp", "rexp"),
    labels = c("Uniform", "Linear asc.", "Linear desc.", "Linear shuffled", "Geometric asc.", "Geometric desc.")))
```


```{r echo=FALSE}
ggplot_perf_base <-
  ggplot_base %>%
  c(list(
    theme_bw(11),
    scale_color_discrete(name = "Algorithm")
  ))

ggplot_time_base <-
  ggplot_perf_base %>%
  c(list(ylab("Run time (s)")))

ggplot_time_per_item_base <-
  ggplot_base %>%
  c(list(ylab("Run time per element (s)")))
```


## Run time

The run time was measured using the \pkg{microbenchmark} package
`r knitcitations::citep(citation("microbenchmark"))`
in block order with a warmup of 10 iterations
using the default 100 iterations.
The tests ran on a single core of an Intel(R) Xeon(R) CPU E5-2643 clocked at 3.30 GHz
with 10 MB cache.
\fxwarning{Which version of the package?}

\Cref{fig:run-time-log} presents an overview of the median run time
for different input sizes, output size ratios, weight distributions and algorithms.
The \proglang{R} implementation is clearly outperformed by all other implementations
for $n = \num{`r max(timings$n)`}$, in many cases even for much smaller inputs.
In the log-log scale used here, the slope of the curves translates to computational complexity;
the steeper slope for the \proglang{R} implementation corresponds to its quadratic complexity
compared to the (in most cases) slightly superlinear complexity of the other algorithms.

```{r run-time-log, echo=FALSE, fig.height=7, fig.cap="Median run times"}
timings %>%
  group_by(n, expr, prob, r) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, color=expr)) +
  scale_y_log10() +
  ggplot_time_base +
  geom_line() +
  scale_x_log10() +
  facet_grid(prob~r) +
  theme(legend.position="bottom")
```

As expected, the *expj* algorithm is among the fastest, especially for $r \ll 1$.
In the case $r = 0.01$ for the *geometric ascending* distribution,
the new implementations win only by a margin;
in particular, the *expj* algorithm depends on the ordering of the weights
which is unfavorable here.

The *rej* and *rank* algorithms exhibit initial costs on the sub-millisecond scale
even for small input sizes, probably due to the fact that both are implemented
purely in \proglang{R}.
In addition, the *rej* algorithm is by far the slowest
(but still asymptotically faster than the stock implementation)
for *geometric* distributions, because in each step only a tiny fraction of items
have a non-negligible weight, and hence most sampled items are rejected as duplicates
(line \ref{alg:sample-rej:sample} of \cref{alg:sample-rej}).

\Cref{fig:run-time-crank-expj} compares run times for *crank* and *expj*
for the different weight distributions,
values above 1 mean that *expj* is faster.
The *expj* algorithm seems to be favorable if $r$ is small or
$n$ is large.
For the pathological *geometric* cases, the run time differences between
*ascending* and *descending* weights are substantial for small $r$.
The advantage of the *expj* algorithm for $r = 1$ and large $n$ is surprising.
\fxwarning{Explain once expj algorithm has been described}

```{r run-time-crank-expj, echo=FALSE, fig.cap="Comparison of \\emph{crank} and \\emph{expj} run times"}
timings %>%
  filter(expr %in% c("crank", "expj")) %>%
  group_by(n, expr, prob, r) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  spread(expr, median) %>%
  mutate(crank_vs_expj = crank / expj) %>%
  ggplot(aes(x=prob, y=crank_vs_expj, color=n)) +
  scale_x_discrete(name = "Weights distribution") +
  scale_y_log10(name = "Ratio of median run times\n(crank vs. expj)", breaks = 2 ** (-1:3)) +
  ggplot_base +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  geom_point() +
  scale_color_continuous(trans="log", breaks = as.integer(10 ** (2:5))) +
  facet_wrap(~r)
```

For the break-even analysis, *expj* is compared to the stock implementation in
\cref{fig:run-time-break-even}.
At $n=1000$, the *expj* algorithm starts to outperform the stock implementation
for all tested values of $r$.
Again, the worst slowdown of the *expj* algorithm seems to be around 2,
for absolute run times of around 10 microseconds for $n = 100$.
\fxwarning{The case $r=0.01$ is unclear.}

```{r run-time-break-even, echo=FALSE, fig.cap="Comparison of \\proglang{R} and \\emph{expj} run times for \\emph{linear ascending} weights"}
break_even %>%
  filter(expr %in% c("R", "expj")) %>%
  filter(prob == "Linear asc.") %>%
  group_by(n, expr, prob, r) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  spread(expr, median) %>%
  mutate(R_vs_expj = R / expj) %>%
  ggplot(aes(x=n, y=R_vs_expj, linetype=factor(r))) +
  ggplot_base +
  geom_line() +
  scale_x_log10() +
  scale_y_log10(breaks = 2 ** (-1:5))
```



## Correctness

A correct implementation should obey the following criteria:

1. All output items are between $1$ and $n$.
2. Each item occurs at most once in the output.\fxwarning{Really check this in \pkg{wrswoR.sample}.}
3. For given parameters $n$, $s$ and $p_i$,
  the probability that item $i$ is at position $j$ in the output
  (with $1 \leq i \leq n$ and $1 \leq j \leq s$)
  is identical to the stock implementation.

Verifying these criteria seems to be challenging
due to the stochasticity of the algorithms.
The first two can be simply checked by observing the output.
Below, a description of the procedure for checking the third criterion is given.

For fixed $i$ and $j$ and for fixed parameters $n$, $s$, and $p_i$,
each call to the sampling routine is a Bernoulli trial
with fixed success probability $\pi_{i,j}$.
Repeated sampling leads to an i.i.d.\ sequence of Bernoulli trials.
In general,
computing the exact value of $\pi_{i,j}$ for large $j$
seems to require considerable computational resources.
Therefore, the value of $\pi_{i,j}$ is assumed unknown,
and only the equality of the proportions is tested for the different algorithms
using a two-sided test for equal proportions (essentially a $\chi^2$ test,
implemented by the `prop.test()` function).
The correctness check is performed as follows:

- The parameters $n$, $s$, and $p_i$, and the algorithm under test, are fixed.
- For both the tested algorithm and for the stock implementation,
  $N$ random samples without replacement are drawn and recorded.
- For all $i$ and $j$, the number of samples where item $i$ is in position $j$
  (denoted by $f_{i,j}$) is computed.
- The counts are tested for equality of proportions,
  yielding a p-value for each tuple $(i, j)$.

In this setting, for fixed $(i, j)$, the p-value is itself a random variable
that is distributed uniformly over $(0, 1]$ under the null hypothesis
of equal proportions (i.e., if the tested implementation is correct).
On the other hand, if one of the algorithms is implemented incorrectly,
the rejection rate for the null hypothesis will be large,
and a substantial share of the p-values will be very close to 0.
While this procedure does not constitute a proof of correctness,
it offers a means to automatically test the implementations for
nontrivial errors.
A similar procedure (using a visual representation with violin plots)
caught an implementation error in the *expj* algorithm
that occurred only in the case $1 < s < n$.

To assert the sensitivity of the testing procedure,
a faulty implementation was simulated by passing altered probabilities to R's
implementation.
The modification consists of updating
$$p_i^\prime \coloneqq p_i \cdot \left(1 + \text{skew} \cdot \frac{i - 1}{n - 1}\right),$$
where a skew of zero means no change, and a skew of 1% corresponds to
relative differences increasing between 0% and 1%.

The test for equal proportions can be substituted by Fisher's exact test,
which tends to produce lower p-values and therefore is usually more powerful
than the test for equal proportions.
However, Fisher's exact test has $O(N)$ complexity,
because it evaluates the density of the hypergeometric distribution
on a support of the order of $N$.
Using this test would have been prohibitive in the setting described here.

```{r def-plot-p-values, echo=FALSE}
n <- 7
s <- 4
skew <- 1.0025
alpha <- 1.08
N <- 2 ** 24

p_values_true <- wrswoR.benchmark::p_values_7 %>% filter(N == global(N), s == global(s), skew == 1, func == "crank")

p_values_false <- wrswoR.benchmark::p_values_7 %>% filter(N == global(N), s == global(s), skew == global(skew))
```

\Cref{fig:correctness-true} shows a Schweder plot [@Schweder_Biometrika_1982]
of the p-values resulting from an experiment that
draws $N = `r paste0("2^{", log2(N), "}")`$ samples for $n = `r n`$,
$s = `r s`$, and a geometric weight distribution with $\alpha = `r alpha`$,
using all five algorithms.
Different values of $i$ and $j$ are denoted with different colors and shapes.
The theoretical distribution is shown as a dotted line,
and aligns very well with the observed p-values.
Fisher's combined probability test is a meta-analysis method that combines
multiple p-values (from different but related studies) into one;
it is implemented
in the \pkg{metap} package `r knitcitations::citep(citation("metap"))`.
For this particular run of the experiment,
Fisher's method cannot reject the null hypothesis of uniformity
($p = `r signif(metap::sumlog(p_values_true$p_value)$p, 3)`$).
As an example for a positive test,
\cref{fig:correctness-false} shows results for the same experiment,
now substituting the stock implementation with a faulty one with
$\text{skew} = `r (skew - 1) * 100`\%$.
Despite the relative similarity of the weight distributions,
the distribution of the p-values deviates substantially from the
uniform distribution,
with more than expected p-values close to zero.
Here, Fisher's method detects significant evidence against the null hypothesis
($p = `r signif(metap::sumlog(p_values_false$p_value)$p, 3)`$).

```{r correctness-true, echo=FALSE, dependson="def-plot-p-values", fig.cap="CDF of the p-values when comparing the five algorithms"}
p_values_true %>%
  rename(`p-value` = p_value) %>%
  arrange(`p-value`) %>%
  mutate(`Rank` = seq_along(`p-value`)) %>% 
  mutate_each(funs(factor), i, j) %>% 
  ggplot(aes(y = `Rank`, x = `p-value`, color = i, shape = j)) +
  ggplot_base +
  geom_point() +
  geom_abline(slope = nrow(p_values_true), intercept = 0, linetype = 3) +
  coord_fixed(ratio = 1 / nrow(p_values_true))
```


```{r correctness-false, echo=FALSE, dependson="def-plot-p-values", fig.cap="CDF of the p-values when comparing five algorithms, where one of which uses different weights"}
p_values_false %>%
  rename(`p-value` = p_value) %>%
  arrange(`p-value`) %>%
  mutate(`Rank` = seq_along(`p-value`)) %>% 
  mutate_each(funs(factor), i, j) %>% 
  ggplot(aes(y = `Rank`, x = `p-value`, color = i, shape = j)) +
  ggplot_base +
  geom_point() +
  geom_abline(slope = nrow(p_values_false), intercept = 0, linetype = 3) +
  coord_fixed(ratio = 1 / nrow(p_values_false))
```

A fairly comprehensive test for
all $n \in \{
`r wrswoR.benchmark::p_values_agg$n %>% range %>% paste(collapse = ", \\dots")`
\}$,
a subset of $s \in \{1, \ldots, n\}$
and all $(i, j)$ was also carried out.
For each combination, the cell frequencies $f_{i,j}$ were collected for all new
algorithms, and the stock implementation with and without altered probabilities
(using $\text{skew}$ values between
$`r 100 * (min(wrswoR.benchmark::p_values_agg$skew[wrswoR.benchmark::p_values_agg$skew > 1]) - 1)` \%$
and
$`r 100 * (max(wrswoR.benchmark::p_values_agg$skew) - 1)` \%$),
for $N$ ranging from $2^{`r log2(min(wrswoR.benchmark::p_values_agg$N))`}$ to
$2^{`r log2(max(wrswoR.benchmark::p_values_agg$N))`}$ (only powers of $2$).
Each cell frequency was compared to that of the stock implementation.
This resulted in around \num{5e8} p-values, which were again
combined using Fisher's method.

\Cref{fig:comprehensive} shows the results of the meta-analysis
separately for each $N$ and for each (supposedly correct or faulty)
implementation.
Comparing the stock implementation to itself resulted in a p-value of almost 1
for all $N$, the same holds for all new algorithms.
On the other hand, all skews tested led to strong rejection of the correctness
hypothesis (p-value effectively 0) sooner or later; as expected,
the smaller the skew, the larger the $N$ where rejection occurs.
This comparison is less sensitive to implementation errors that occur only for
specific arguments
(e.g., if an implementation behaves as expected except if $n$ is a power of 2).
\Cref{fig:comprehensive-true,fig:comprehensive-false} demonstrate p-values
further disaggregated by $n$.

- Correct implementations are all blue, faulty implementations turn grey sooner or later.
- For small skew cannot conclusively disprove correctness for all $n$, need more data.
- A few dark spots with correct implementation, but not more than expected (each meta-analysis is an analysis in itself)
- Show which tiles in the heatmap correspond to the plots


```{r comprehensive, echo=FALSE, fig.cap="Combining p-values for a comprehensive test"}
wrswoR.benchmark::p_values_agg_agg %>%
  ggplot(aes(x=factor(N), y=interaction(func, skew), fill=p_value)) +
  ggplot_base +
  geom_tile() +
  scale_fill_continuous(limits = c(1e-4, 1), trans = "log10")
```

```{r comprehensive-true, echo=FALSE, fig.cap="CDF of the p-values when comparing five algorithms, where one of which uses different weights"}
wrswoR.benchmark::p_values_agg %>%
  filter(skew == 1 & func != "R") %>%
  ggplot(aes(x=factor(N), y=n, fill=p_value)) +
  ggplot_base +
  geom_tile() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_continuous(limits = c(1e-4, 1), trans = "log10") +
  facet_wrap(~func)
```

```{r comprehensive-false, echo=FALSE, fig.cap="CDF of the p-values when comparing five algorithms, where one of which uses different weights"}
wrswoR.benchmark::p_values_agg %>%
  filter(func == "R") %>%
  ggplot(aes(x=factor(N), y=n, fill=p_value)) +
  ggplot_base +
  geom_tile() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_continuous(limits = c(1e-4, 1), trans = "log10") +
  facet_wrap(~skew, ncol = 4)
```



# Conclusions and future work

- Pre-sort weights using radix sort
    - For expj
    - Sorting by exponent may already be enough
- Faster heap (g-heap?)
- Generic C library
- Allow termination of loop in R
- Test more failure modes
- Parallel version

This would have been much more difficult without the \pkg{BatchJobs} and
\pkg{BatchExperiment} packages
`r knitcitations::citep(c(citation("BatchJobs"), citation("BatchExperiments")))`.


# References {.unnumbered}

\justifying

```{r echo=FALSE, message=FALSE, cache=FALSE}
knitcitations::write.bibtex()
```
