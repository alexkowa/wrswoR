---
author:
  - name: Kirill Müller
    affiliation: IVT, ETH Zurich
    address: >
      Stefano-Franscini-Platz 9
      8093 Zürich
    email: kirill.mueller@ivt.baug.ethz.ch
    url: http://www.ivt.ethz.ch
title:
  formatted: "Accelerating weighted random sampling without replacement"
  # If you use tex in the formatted title, also supply version without
  plain:     "Accelerating weighted random sampling without replacement"
  # For running headers, if needed
  short:     "Accelerating weighted random sampling without replacement"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{R}"]
  plain:     [keywords, not capitalized, R]
preamble: >
  \usepackage{amsmath}
  \usepackage[USenglish]{babel}
  \usepackage{algorithm}
  \usepackage{algorithmic}
  \usepackage[draft]{fixme}
  \usepackage[capitalize]{cleveref}
  \usepackage{ragged2e}
  \usepackage{mathtools}
output: rticles::jss_article
bibliography: [ my.bib, knitcitations.bib ]
vignette: >
  %\VignetteIndexEntry{Accelerating weighted random sampling without replacement}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r echo=FALSE, message=FALSE}
library(wrswoR)
import::from(tidyr, spread, gather, gather_, .library = .libPaths())
import::from(plyr, ldply, .library = .libPaths())
import::from(dplyr, filter, mutate, select, group_by, summarize, ungroup, tbl_df,
             rename, arrange, mutate_each, funs, .library = .libPaths())
import::from(cluster, daisy, .library = .libPaths())
import::from(magrittr, "%>%", .library = .libPaths())
library(ggplot2)
set.seed(20150710L)
knitr::opts_chunk$set(cache=TRUE)
knitcitations::cite_options("pandoc")

knit_print.function <- function(x, options) {
  dput(x)
}
```

# Introduction

\fxwarning{(Everywhere) Refer to weights and not probabilities}

Random sampling is one of the basic primitives in statistical computing.
\fxwarning{Used where?}

This paper focuses on a specific variant: sampling without replacement from a finite population
with non-uniform probability distribution.
\fxwarning{Application?}

First, different techniques for sampling from discrete populations are reviewed.
Several implementations for sampling without replacement are discussed,
this includes evaluation of correctness and runtime performance.
The paper concludes with suggestions for incorporating the findings into base
\proglang{R}.


# Sampling from discrete populations

We use \cref{alg:sample} as a definition of sampling from discrete populations
with or without replacement from arbitrary (including uniform) probability distributions.
From this definition, the following can be observed:

\begin{algorithm}
  \caption{$\text{sample}(n, s, \text{replace}, p_i)$}
  \label{alg:sample}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $\text{replace}$: \TRUE{} to request sampling with replacement
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\} $ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \IF{$s = 0$}
      \RETURN vector of length 0
    \ENDIF
    \STATE Randomly select $k$ so that $\mathrm{P}(k=i) = \frac{p_k}{\sum_{j}p_j}$
      for all $i$ \label{alg:sample:norm}
    \IF{not replace}\label{alg:sample:if}
      \STATE $n \leftarrow n - 1$
      \STATE remove item $k$ from $p_i$\label{alg:sample:remove}
    \ENDIF
    \RETURN $k \oplus \text{sample}(n, s - 1, \text{replace}, p_i)$
  \end{algorithmic}
\end{algorithm}

- Sampling with replacement appears to be a simpler problem than sampling without
  replacement, as the block starting at line \ref{alg:sample:if} is not required.
- If all probabilities $p_i$ are equal, the selection probability $\mathrm{P}(i=k)$
  of the sampled items in line \ref{alg:sample:norm} always equals $\frac{1}{n}$
  and does not have to be computed explicitly.

In this framework, sampling without replacement with non-uniform probabilities
is the hardest problem.
This intuition carries over to the more specialized algorithms that implement
each case.


## With replacement

The *with replacement* case corresponds to repeated selection of $k$ from *the same*
discrete probability distribution.
The uniform case can be implemented easily by scaling and discretizing the
output of a random number generator.
More work is needed in the non-uniform case.
Assuming w.l.o.g.\ $\sum_j p_j = n$,
it is possible to construct (in $O(n)$ time) a subdivision
$(l_i, r_i, s_i)$ with $i, l_i, r_i \in \{1,\ldots,n\}$
and $0 < s_i \leq 1$ so that $$p_i = \sum_{j:l_j=i}{s_j} + \sum_{j:r_j=i}{(1-s_j)}$$
[@Walker__1977].
Choosing an item requires sampling from $\{1,\ldots,n\}$ (to choose $i$) 
and then sampling from $\left[0, 1\right)$ (to choose $l_i$ or $r_i$).
(Figuratively, the probability mass given by $p_i$
is distributed over $n$ "boxes" so that the space in each box $i$
is assigned to at most two items
$l_i$ and $r_i$.
The share occupied by item $l_i$ in box $i$ is given by $s_i$.
Some items can be distributed over several boxes.
Choosing an item means selecting a box and choosing between the two items in this box.)

The preprocessing time of $O(n)$ required for Walker's alias method
is also the lower bound for arbitrary probability distributions.
It is easy to see that the entire probability vector must be read at least once.
Hence, for nonuniform probabilities,
the run time is at least $O(n + s)$, and the input size $n$ will dominate
unless $s \gg O(n)$.


## Without replacement

In the *without replacement* case, each selected item is removed from the
collection of candidate items.
Again, the uniform case is much simpler.
An unordered array of size $n$, initialized with the natural sequence,
can be used for storing the candidate items.
The selection of the item corresponds to choosing an index at random in this
unordered array.
Removal of an item with known index can be done in $O(1)$ time.
\fxwarning{Reservoir for $s \ll n$}
For the non-uniform case,
lines \ref{alg:sample:norm} and \ref{alg:sample:remove}
in \cref{alg:sample} can be implemented with
a data structure that maintains a subdivision of an interval
into $n$ subintervals and allows lookups and updates.
Walker's alias method seems to be ill-suited for this purpose,
as each item potentially spreads over several "boxes",
which can make updates costly.
@Wong__1980 propose a data structure similar to a heap
that can be initialized in $O(n)$ time and supports
simultaneous lookup and update in $O(\log n)$ time.

\proglang{R} offers efficient implementations for all cases except
non-uniform sampling without replacement.


\fxwarning{introduce next section}

# Explanation of the code

This paper explores alternative approaches:
rejection sampling and reservoir sampling.
Only the former can be described formally within the framework of \cref{alg:sample},
however an actual implementation would use sampling *with* replacement as a subroutine.
The latter is based on arithmetic transformation of a probability distribution.


## Rejection sampling

Sampling without replacement can be emulated by sampling with replacement.
In the framework of \cref{alg:sample},
this corresponds to flagging sampled items as "invalid" (instead of removing them)
and repeating the sampling in line \ref{alg:sample:norm}
until hitting a valid item.
Here, rejection sampling is used to sample a valid item from a (larger)
set of potentially invalid items.
Note that the distribution of the result is not modified if invalid items
are purged occasionally.

This approach can be implemented by repeatedly calling a subroutine that
returns a sample with replacement, as shown in \cref{alg:sample-rej}.
\fxwarning{Reference?}
The general idea is to sample slightly more items than necessary (with replacement),
and then to throw away the duplicate items.
If the resulting sequence of items is shorter than requested,
the result for a much smaller problem is appended to it.
In \cref{alg:sample-rej},
duplicate items in the result of a sampling with replacement
(line \ref{alg:sample-rej:sample})
correspond to invalid items in the rejection sampling,
and the recursive call in line \ref{alg:sample-rej:rec} corresponds to
purging the invalid items.



\begin{algorithm}
  \caption{$\text{sample.rej}(n, s, p_i)$}
  \label{alg:sample-rej}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\} $ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \STATE $k_i \leftarrow \text{unique}(\text{sample}(n, \text{expected.items}(n, s), \TRUE, p_i))$
      \label{alg:sample-rej:sample}
    \STATE $l \leftarrow \text{length}(k_i)$
    \IF{$l \geq s$}
      \RETURN the first $s$ items of $k_i$
    \ENDIF
    \STATE remove items $k_i$ from $p_i$\label{alg:sample-rej:remove}
    \RETURN $k_i \oplus \text{sample.rej}(n - l, s - l, p_i)$\label{alg:sample-rej:rec}
  \end{algorithmic}
\end{algorithm}

Here, the function expected.items is supposed to evaluate to a reasonable estimate
of the number of items that need to be drawn with replacement,
so that the result can be expected to contain at least $s$ unique items.
(The correctness of the estimate only affects the run time, not the correctness
of the algorithm.)
For a uniform distribution, it can be shown that, with
$\text{expected.items}(n, s) \geq n (H_n - H_{n-s}) = n \sum_{i=n-s+1}^n \frac{1}{i}$,
the result has $s$ unique items in expectation.
\fxnote{Expected amortized run time? Look it up in a reference}
With $\text{expected.items}(n, s) \equiv 1$,
\cref{alg:sample,alg:sample-rej} are in fact identical.
A formal proof for the more interesting case $\text{expected.items}(n, s) > 1$
is beyond the scope of this paper.


## Reservoir sampling

A particularly interesting algorithm has been devised only recently by
@efraimidis_weighted_2006.
In the simplest version, it is sufficient to draw $n$ random numbers,
combine them arithmetically with the probability distribution $p_i$,
and perform a partial sort to find the indexes of the $s$ smallest items.
\Cref{alg:sample-rank} is a modified version of Algorithm A in the original paper
that operates on the logarithmic scale and uses multiplication
instead of exponentiation
for increased numeric stability.
\fxnote{Describe transformation}
In line \ref{alg:sample-rank:sample}, $\text{Exp}(1)$ refers to i.i.d.\ samples
from the exponential distribution with rate 1.
\fxnote{Logarithm of the uniform}

\begin{algorithm}
  \caption{$\text{sample.rank}(n, s, p_i)$}
  \label{alg:sample-rank}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\}$ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \STATE $r_i \leftarrow \text{Exp}(1) / p_i$ for all $i \in \{1,\ldots,n\}$
      \label{alg:sample-rank:sample}
    \RETURN the positions of the $s$ smallest elements in $r_i$\label{alg:sample-rank:rec}
  \end{algorithmic}
\end{algorithm}

The algorithm amazes with its elegance and simplicity.
Computational complexity is dominated by the partial sort
(which can be implemented in $O(n + s \log n)$,
or even in $O(n)$ for floating-point numbers
[@terdiman_radix_2000].
However, generating random variates is computationally expensive,
and for $s \ll n$ this may dominate.
\fxnote{Parallelize}.
To overcome this issue in this case,
@efraimidis_weighted_2006
describe *reservoir sampling with exponential jumps* -- 
an extension where each generated random number decides how many items are skipped
until the current "least likely" item is removed from the reservoir.
Only $O(s \log \frac{n}{s})$ random numbers (in expectation) are needed with this extension,
the simple version always requires $n$ random numbers.
\fxwarning{Show algorithm}
We refer to the original paper for more details, including formal proofs of correctness.


# Code

The \pkg{wrswoR} package contains implementations for the two algorithms
presented in the previous section, one \proglang{R} implementation
of rejection sampling (\cref{alg:sample-rej}, denoted by *rej*),
two implementations (\proglang{R} and \proglang{C++})
of simplified reservoir sampling (\cref{alg:sample-rank}, *rank* and *crank*)
and one \proglang{C++} implementation of reservoir sampling with exponential jumps
(*expj*).
In the package, the corresponding functions are prefixed with \code{sample_int_}.
The \pkg{Rcpp} package
`r knitcitations::citep(citation("Rcpp"))`
is used to generate the glue between \proglang{R} and \proglang{C++}.

The \proglang{R} implementations are very similar to the pseudocode:
As an example, the *rank* implementation is shown below.

```{r, echo=FALSE}
sample_int_rank
```

The function arguments correspond to those of \cref{alg:sample,alg:sample-rej,alg:sample-rank}:
\code{size} is the $s$ argument, and \code{prob} is the $p_i$ argument.

The *crank* implementation has been somewhat optimized for cache efficiency.
Due to its relative complexity, the *expj* implementation is kept very close
to the pseudocode in the original paper,
still this function also operates on the logarithmic scale for numeric stability.
The transformation works in a fashion very similar to that of
\cref{alg:sample-rank}.

All functions share the same interface.
Compared to \code{sample.int}, the base \proglang{R} function,
the \code{replace} parameter has been removed, and \code{prob} cannot be \code{NULL}.
To simplify testing the new routines against the \proglang{R} implementation,
a wrapper function \code{sample_int_R} is provided.
The subsequent section shows performance characteristics and correctness
of the new implementations compared to the base \proglang{R} version.


# Tests

This section presents run time tests for various combinations of input parameters,
attempts to provide guidance when to choose which implementation,
and discusses the correctness of the implementation.
All test results shown in this section are based on data available
in the \pkg{wrswoR.benchmark} package.


## Input parameters

The run time tests used different values for the function arguments
$n$, $s$ and $prob$.
Instead of directly specifying $s$, it is given as a fraction of $n$,
denoted by $r = \frac{s}{n}$.
The following weight distributions (used for $p_i$) were tested:

- *Uniform*
- *Linear*: Sequence from $1$ to $n$ ($\{p_i\} = \{1,\ldots n\}$, *ascending*, *descending* and *shuffled*)
- *Geometric*: Starting at $1$, the weight is multiplied with a constant $\alpha$ for each step
  ($p_{i+1} = \alpha p_i$, *ascending* and *descending*);
  the constant is chosen so that both minimal and maximal weights
  and the sum of weights is still representable as a floating-point number.
  \fxwarning{Extreme case}

```{r echo=FALSE}
ggplot_base <- list(
  theme_bw(11)
)

ALGOS <- c("R", "rej", "rank", "crank", "expj")

REMOVE_PROB <- c("linear_double", "linear_half")

timings <- 
  wrswoR.benchmark::timings %>%
  tbl_df %>%
  filter(expr %in% ALGOS) %>%
  filter(!(prob %in% REMOVE_PROB)) %>%
  mutate(expr = factor(expr, levels = ALGOS)) %>%
  mutate(prob = factor(
    prob, levels = c("uniform", "linear", "rev_linear", "shuffled_linear", "exp", "rexp"),
    labels = c("Uniform", "Linear asc.", "Linear desc.", "Linear shuffled", "Geometric asc.", "Geometric desc.")))

BASE <- 1.007
N <- max(timings$n)
```

```{r echo=FALSE}
break_even <- 
  wrswoR.benchmark::break_even %>%
  tbl_df %>%
  filter(expr %in% ALGOS) %>%
  filter(!(prob %in% REMOVE_PROB)) %>%
  mutate(expr = factor(expr, levels = ALGOS)) %>%
  mutate(prob = factor(
    prob, levels = c("uniform", "linear", "rev_linear", "shuffled_linear", "exp", "rexp"),
    labels = c("Uniform", "Linear asc.", "Linear desc.", "Linear shuffled", "Geometric asc.", "Geometric desc.")))
```


```{r echo=FALSE}
ggplot_perf_base <-
  ggplot_base %>%
  c(list(
    theme_bw(11),
    scale_color_discrete(name = "Algorithm")
  ))

ggplot_time_base <-
  ggplot_perf_base %>%
  c(list(ylab("Run time (s)")))

ggplot_time_per_item_base <-
  ggplot_base %>%
  c(list(ylab("Run time per element (s)")))
```


## Run time

The run time was measured using the \pkg{microbenchmark} package
`r knitcitations::citep(citation("microbenchmark"))`
in block order with a warmup of 10 iterations
using the default 100 iterations.
The tests ran on a single core of an Intel(R) Xeon(R) CPU E5-2643 clocked at 3.30 GHz
with 10 MB cache.
\fxwarning{Which version of the package?}

\Cref{fig:run-time-log} presents an overview of the median run time
for different input sizes, output size ratios, weight distributions and algorithms.
The \proglang{R} implementation is clearly outperformed by all other implementations
for $n = `r max(timings$n)`$, in many cases even for much smaller inputs.
In the log-log scale used here, the slope of the curves translates to computational complexity;
the steeper slope for the \proglang{R} implementation corresponds to its quadratic complexity
compared to the (in most cases) slightly superlinear complexity of the other algorithms.

```{r run-time-log, echo=FALSE, fig.height=7, fig.cap="Median run times"}
timings %>%
  group_by(n, expr, prob, r) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, color=expr)) +
  scale_y_log10() +
  ggplot_time_base +
  geom_line() +
  scale_x_log10() +
  facet_grid(prob~r) +
  theme(legend.position="bottom")
```

As expected, the *expj* algorithm is among the fastest, especially for $r \ll 1$.
In the case $r = 0.01$ for the *geometric ascending* distribution,
the new implementations win only by a margin;
in particular, the *expj* algorithm depends on the ordering of the weights
which is unfavorable here.

The *rej* and *rank* algorithms exhibit initial costs on the sub-millisecond scale
even for small input sizes, probably due to the fact that both are implemented
purely in \proglang{R}.
In addition, the *rej* algorithm is by far the slowest
(but still faster than the stock implementation)
for *geometric* distributions, because in each step only a tiny fraction of items
have a non-negligible weight, and hence most sampled items are rejected as duplicates
(line \ref{alg:sample-rej:sample} of \cref{alg:sample-rej}).

\Cref{fig:run-time-crank-expj} compares run times for *crank* and *expj*
for the different weight distributions,
values above 1 mean that *expj* is faster.
The *expj* algorithm seems to be favorable if $r$ is small or
$n$ is large.
For the pathological *geometric* cases, the run time differences between
*ascending* and *descending* weights are substantial for small $r$.
The advantage of the *expj* algorithm for $r = 1$ and large $n$ is surprising.
\fxwarning{Explain once expj algorithm has been described}

```{r run-time-crank-expj, echo=FALSE, fig.cap="Comparison of \\emph{crank} and \\emph{expj} run times"}
timings %>%
  filter(expr %in% c("crank", "expj")) %>%
  group_by(n, expr, prob, r) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  spread(expr, median) %>%
  mutate(crank_vs_expj = crank / expj) %>%
  ggplot(aes(x=prob, y=crank_vs_expj, color=n)) +
  scale_x_discrete(name = "Weights distribution") +
  scale_y_log10(name = "Ratio of median run times\n(crank vs. expj)", breaks = 2 ** (-1:3)) +
  ggplot_base +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  geom_point() +
  scale_color_continuous(trans="log", breaks = as.integer(10 ** (2:5))) +
  facet_wrap(~r)
```

For the break-even analysis, *expj* is compared to the stock implementation in
\cref{fig:run-time-break-even}.
At $n=1000$, the *expj* algorithm starts to outperform the stock implementation
for all tested values of $r$.
Again, the worst slowdown of the *expj* algorithm seems to be around 2,
for absolute run times of around 10 microseconds for $n = 100$.

```{r run-time-break-even, echo=FALSE, fig.cap="Comparison of \\proglang{R} and \\emph{expj} run times for \\emph{linear ascending} weights"}
break_even %>%
  filter(expr %in% c("R", "expj")) %>%
  filter(prob == "Linear asc.") %>%
  group_by(n, expr, prob, r) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  spread(expr, median) %>%
  mutate(R_vs_expj = R / expj) %>%
  ggplot(aes(x=n, y=R_vs_expj, linetype=factor(r))) +
  ggplot_base +
  geom_line() +
  scale_x_log10() +
  scale_y_log10(breaks = 2 ** (-1:5))
```



## Correctness

Verifying the correctness of the implementation seems to be challenging
due to the stochasticity of the algorithms.
Of course, all output items should be between $1$ and $n$,
and each item should occur at most once in the output;
this is verified easily.
In addition, for given parameters $n$, $s$ and $p_i$,
the probability that item $i$ is at position $j$ in the output
(with $1 \leq i \leq n$ and $1 \leq j \leq s$) should be identical for all implementations.

For fixed $i$ and $j$ and for fixed parameters $n$, $s$, and $p_i$,
each call to the sampling routine is a Bernoulli trial
with fixed success probability $\pi_{i,j}$.
Repeated sampling leads to an i.i.d.\ sequence of Bernoulli trials.
In general,
computing the exact value of $\pi_{i,j}$ for large $j$
seems to require considerable computational resources.
Therefore, the value of $\pi_{i,j}$ is assumed unknown,
and only the equality of the proportions is tested for the different algorithms
using Fisher's exact test.
The validation is performed as follows:

- The parameters $n$, $s$, and $p_i$ are fixed.
- For all algorithms, $N$ random samples without replacement are drawn and recorded.
- For all $i$ and $j$, the number of samples where item $i$ is in position $j$
  (denoted by $f_{i,j}$) is computed.
- The counts (one per algorithm, $f_{i,j}$ successes out of $N$ trials)
  are tested for equality of proportions, yielding a p-value for each tuple $(i, j)$.

In this setting, for fixed $(i, j)$, the p-value is itself a random variable
that is distributed uniformly over $(0, 1]$ under the null hypothesis
of equal proportions (i.e., if all implementations are correct).
On the other hand, if one of the algorithms is implemented incorrectly,
the rejection rate for the null hypothesis will be large,
and a substantial share of the p-values will be very close to 0.
While this procedure does not constitute a proof of correctness,
it offers a means to automatically test the implementations for
nontrivial errors.
A similar procedure (using a visual representation with violin plots)
caught an implementation error in the *expj* algorithm
that occurred only in the case $1 < s < n$.

```{r def-plot-p-values, echo=FALSE}
n <- 7
s <- 4
alpha <- 1.08
N <- 2 ** 23

p_values_true <- wrswoR.benchmark::p_values_7 %>% filter(N == global(N), s == global(s), skew == 1)

p_values_false <- wrswoR.benchmark::p_values_7 %>% filter(N == global(N), s == global(s), skew == 1.005)
```

\Cref{fig:correctness-true} shows the empirical CDF of the p-values
when drawing $N = `r paste0("2^{", log2(N), "}")`$ samples for $n = `r n`$,
$s = `r s`$, and a geometric weight distribution with $\alpha = `r alpha`$.
Different values of $i$ and $j$ are denoted with different colors and point shapes.
The theoretical distribution is shown as a dotted line,
and aligns very well with the observed p-values.
For this particular run of the experiment,
Fisher's combined probability test cannot reject the null hypothesis of uniformity
($p = `r round(metap::sumlog(p_values_true$p_value)$p, 3)`$).
As an example for a positive test, 
\cref{fig:correctness-false} shows results for the same experiment,
now comparing different weight distributions (uniform and geometric)
using only the stock implementation.
Despite the relatively small size of the experiment
and the relative similarity of the weight distributions,
the distribution of the p-values deviates substantially from the
theoretical distribution.
Here, Fisher's method detects significant evidence against the null hypothesis
($p = `r round(metap::sumlog(p_values_false$p_value)$p, 4)`$).

```{r correctness-true, echo=FALSE, dependson="def-plot-p-values", fig.cap="CDF of the p-values when comparing the five algorithms"}
p_values_true %>%
  rename(`p-value` = p_value) %>%
  arrange(`p-value`) %>%
  mutate(`Cumulative density` = (seq_along(`p-value`) - 1) / (length(`p-value`) - 1)) %>% 
  mutate_each(funs(factor), i, j) %>% 
  ggplot(aes(y = `Cumulative density`, x = `p-value`, color = i)) +
  ggplot_base +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = 3) +
  coord_fixed()
```


```{r correctness-false, echo=FALSE, dependson="def-plot-p-values", fig.cap="CDF of the p-values when comparing different weight distributions"}
p_values_false %>%
  rename(`p-value` = p_value) %>%
  arrange(`p-value`) %>%
  mutate(`Cumulative density` = (seq_along(`p-value`) - 1) / (length(`p-value`) - 1)) %>% 
  mutate_each(funs(factor), i, j) %>% 
  ggplot(aes(y = `Cumulative density`, x = `p-value`, color = i, shape = j)) +
  ggplot_base +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = 3) +
  coord_fixed()
```

```{r}
#p_values_df <- wrswoR.benchmark::p_values_df( %>%
#  filter(!(prob_distr %in% c("linear_double", "linear_half")))

#p_values_df_false <- wrswoR.benchmark::p_values_df_false
```

An exhaustive test for all algorithms and all $(i, j)$ was also carried out.
For all $n \in \{`r 2`, \ldots,
`r 80`\}$,
a fraction of  $s \in \{1, \ldots, n\}$
and all probability distributions discussed above (with $\alpha = 1.08$),
the p-values
the 5%-quantiles of the p-values were between 





- Perform tests with larger ranges: Accumulate KS-test p-values of p-values distribution

The p-values can be combined using Fisher's method
with the help of the \pkg{metap} package `r knitcitations::citep(citation("metap"))`.
To reduce simulation effort,
p-values resulting from different $(i,j)$
or even from different $n$, $s$, and $p_i$ parameters can be pooled
before testing uniformity.




# Conclusions and future work

- Pre-sort weights using radix sort
    - Sorting by exponent may already be enough
- Faster heap (g-heap?)
- Generic C library
- Allow termination of loop in R

# References {.unnumbered}

\justifying

```{r echo=FALSE, message=FALSE, cache=FALSE}
knitcitations::write.bibtex()
```
