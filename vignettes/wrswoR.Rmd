---
author:
  - name: Kirill Müller
    affiliation: IVT, ETH Zurich
    address: >
      Stefano-Franscini-Platz 9
      8093 Zürich
    email: kirill.mueller@ivt.baug.ethz.ch
    url: http://www.ivt.ethz.ch
title:
  formatted: "Accelerating weighted random sampling without replacement"
  # If you use tex in the formatted title, also supply version without
  plain:     "Accelerating weighted random sampling without replacement"
  # For running headers, if needed
  short:     "Accelerating weighted random sampling without replacement"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{R}"]
  plain:     [keywords, not capitalized, R]
preamble: >
  \usepackage{amsmath}
  \usepackage[USenglish]{babel}
  \usepackage{algorithm}
  \usepackage{algorithmic}
  \usepackage[draft]{fixme}
  \usepackage[capitalize]{cleveref}
  \usepackage{ragged2e}
output: rticles::jss_article
bibliography: [ my.bib, knitcitations.bib ]
vignette: >
  %\VignetteIndexEntry{Accelerating weighted random sampling without replacement}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r echo=FALSE, message=FALSE}
library(wrswoR)
import::from(tidyr, spread, gather, gather_)
import::from(plyr, ldply)
import::from(dplyr, filter, mutate, select, group_by, summarize, ungroup, tbl_df)
import::from(cluster, daisy, .library = .libPaths())
import::from(magrittr, "%>%")
library(ggplot2)
set.seed(20150710L)
knitr::opts_chunk$set(cache=TRUE)
knitcitations::cite_options("pandoc")
```

# Introduction

\fxwarning{(Everywhere) Refer to weights and not probabilities}

Random sampling is one of the basic primitives in statistical computing.
\fxwarning{Used where?}

This paper focuses on a specific variant: sampling without replacement from a finite population
with non-uniform probability distribution.
\fxwarning{Application?}

First, different techniques for sampling from discrete populations are reviewed.
Several implementations for sampling without replacement are discussed,
this includes evaluation of correctness and runtime performance.
The paper concludes with suggestions for incorporating the findings into base
\proglang{R}.


# Sampling from discrete populations

We use \cref{alg:sample} as a definition of sampling from discrete populations
with or without replacement from arbitrary (including uniform) probability distributions.
From this definition, the following can be observed:

\begin{algorithm}
  \caption{$\text{sample}(n, s, \text{replace}, p_i)$}
  \label{alg:sample}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $\text{replace}$: \TRUE{} to request sampling with replacement
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\} $ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \IF{$s = 0$}
      \RETURN vector of length 0
    \ENDIF
    \STATE Randomly select $k$ so that $\mathrm{P}(k=i) = \frac{p_k}{\sum_{j}p_j}$
      for all $i$ \label{alg:sample:norm}
    \IF{not replace}\label{alg:sample:if}
      \STATE $n \leftarrow n - 1$
      \STATE remove item $k$ from $p_i$\label{alg:sample:remove}
    \ENDIF
    \RETURN $k \oplus \text{sample}(n, s - 1, \text{replace}, p_i)$
  \end{algorithmic}
\end{algorithm}

- Sampling with replacement appears to be a simpler problem than sampling without
  replacement, as the block starting at line \ref{alg:sample:if} is not required.
- If all probabilities $p_i$ are equal, the selection probability $\mathrm{P}(i=k)$
  of the sampled items in line \ref{alg:sample:norm} always equals $\frac{1}{n}$
  and does not have to be computed explicitly.

In this framework, sampling without replacement with non-uniform probabilities
is the hardest problem.
This intuition carries over to the more specialized algorithms that implement
each case.


## With replacement

The *with replacement* case corresponds to repeated selection of $k$ from *the same*
discrete probability distribution.
The uniform case can be implemented easily by scaling and discretizing the
output of a random number generator.
More work is needed in the non-uniform case.
Assuming w.l.o.g.\ $\sum_j p_j = n$,
it is possible to construct (in $O(n)$ time) a subdivision
$(l_i, r_i, s_i)$ with $i, l_i, r_i \in \{1,\ldots,n\}$
and $0 < s_i \leq 1$ so that $$p_i = \sum_{j:l_j=i}{s_j} + \sum_{j:r_j=i}{(1-s_j)}$$
[@Walker__1977].
Choosing an item requires sampling from $\{1,\ldots,n\}$ (to choose $i$) 
and then sampling from $\left[0, 1\right)$ (to choose $l_i$ or $r_i$).
(Figuratively, the probability mass given by $p_i$
is distributed over $n$ "boxes" so that the space in each box $i$
is assigned to at most two items
$l_i$ and $r_i$.
The share occupied by item $l_i$ in box $i$ is given by $s_i$.
Some items can be distributed over several boxes.
Choosing an item means selecting a box and choosing between the two items in this box.)

The preprocessing time of $O(n)$ required for Walker's alias method
is also the lower bound for arbitrary probability distributions.
It is easy to see that the entire probability vector must be read at least once.
Hence, for nonuniform probabilities,
the run time is at least $O(n + s)$, and the input size $n$ will dominate
unless $s \gg O(n)$.


## Without replacement

In the *without replacement* case, each selected item is removed from the
collection of candidate items.
Again, the uniform case is much simpler.
An unordered array of size $n$, initialized with the natural sequence,
can be used for storing the candidate items.
The selection of the item corresponds to choosing an index at random in this
unordered array.
Removal of an item with known index can be done in $O(1)$ time.
\fxwarning{Reservoir for $s \ll n$}
For the non-uniform case,
lines \ref{alg:sample:norm} and \ref{alg:sample:remove}
in \cref{alg:sample} can be implemented with
a data structure that maintains a subdivision of an interval
into $n$ subintervals and allows lookups and updates.
Walker's alias method seems to be ill-suited for this purpose,
as each item potentially spreads over several "boxes",
which can make updates costly.
@Wong__1980 propose a data structure similar to a heap
that can be initialized in $O(n)$ time and supports
simultaneous lookup and update in $O(\log n)$ time.

\proglang{R} offers efficient implementations for all cases except
non-uniform sampling without replacement.


\fxwarning{introduce next section}

# Explanation of the code

This paper explores alternative approaches:
rejection sampling and reservoir sampling.
Only the former can be described formally within the framework of \cref{alg:sample},
however an actual implementation would use sampling *with* replacement as a subroutine.
The latter is based on arithmetic transformation of a probability distribution.


## Rejection sampling

Sampling without replacement can be emulated by sampling with replacement.
In the framework of \cref{alg:sample},
this corresponds to flagging sampled items as "invalid" (instead of removing them)
and repeating the sampling in line \ref{alg:sample:norm}
until hitting a valid item.
Here, rejection sampling is used to sample a valid item from a (larger)
set of potentially invalid items.
Note that the distribution of the result is not modified if invalid items
are purged occasionally.

This approach can be implemented by repeatedly calling a subroutine that
returns a sample with replacement, as shown in \cref{alg:sample-rej}.
\fxwarning{Reference?}
The general idea is to sample slightly more items than necessary (with replacement),
and then to throw away the duplicate items.
If the resulting sequence of items is shorter than requested,
the result for a much smaller problem is appended to it.
In \cref{alg:sample-rej},
duplicate items in the result of a sampling with replacement
(line \ref{alg:sample-rej:sample})
correspond to invalid items in the rejection sampling,
and the recursive call in line \ref{alg:sample-rej:rec} corresponds to
purging the invalid items.



\begin{algorithm}
  \caption{$\text{sample.rej}(n, s, p_i)$}
  \label{alg:sample-rej}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\} $ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \STATE $k_i \leftarrow \text{unique}(\text{sample}(n, \text{expected.items}(n, s), \TRUE, p_i))$
      \label{alg:sample-rej:sample}
    \STATE $l \leftarrow \text{length}(k_i)$
    \IF{$l \geq s$}
      \RETURN the first $s$ items of $k_i$
    \ENDIF
    \STATE remove items $k_i$ from $p_i$\label{alg:sample-rej:remove}
    \RETURN $k_i \oplus \text{sample.rej}(n - l, s - l, p_i)$\label{alg:sample-rej:rec}
  \end{algorithmic}
\end{algorithm}

Here, the function expected.items is supposed to evaluate to a reasonable estimate
of the number of items that need to be drawn with replacement,
so that the result can be expected to contain at least $s$ unique items.
(The correctness of the estimate only affects the run time, not the correctness
of the algorithm.)
For a uniform distribution, it can be shown that, with
$\text{expected.items}(n, s) \geq n (H_n - H_{n-s}) = n \sum_{i=n-s+1}^n \frac{1}{i}$,
the result has $s$ unique items in expectation.
\fxnote{Expected amortized run time? Look it up in a reference}
With $\text{expected.items}(n, s) \equiv 1$,
\cref{alg:sample,alg:sample-rej} are in fact identical.
A formal proof for the more interesting case $\text{expected.items}(n, s) > 1$
is beyond the scope of this paper.


## Reservoir sampling

A particularly interesting algorithm has been devised only recently by
@efraimidis_weighted_2006.
In the simplest version, it is sufficient to draw $n$ random numbers,
combine them arithmetically with the probability distribution $p_i$,
and perform a partial sort to find the indexes of the $s$ smallest items.
\Cref{alg:sample-rank} is a modified version of Algorithm A in the original paper
that operates on the logarithmic scale and uses multiplication
instead of exponentiation
for increased numeric stability.
\fxnote{Describe transformation}
In line \ref{alg:sample-rank:sample}, $\text{Exp}(1)$ refers to i.i.d.\ samples
from the exponential distribution with rate 1.
\fxnote{Logarithm of the uniform}

\begin{algorithm}
  \caption{$\text{sample.rank}(n, s, p_i)$}
  \label{alg:sample-rank}
  \begin{algorithmic}[1]
    \REQUIRE $n$: Size of the population
    \REQUIRE $s$: Number of items to sample
    \REQUIRE $p_i$: Relative probability of each item for $i \in \{1,\ldots,n\}$
    \ENSURE Returns a vector $k_j \in \{1,\ldots,n\}$ with
      $j \in \{1,\ldots,s\}$ that contains the indexes of the items sampled
    \STATE $r_i \leftarrow \text{Exp}(1) / p_i$ for all $i \in \{1,\ldots,n\}$
      \label{alg:sample-rank:sample}
    \RETURN the positions of the $s$ smallest elements in $r_i$\label{alg:sample-rank:rec}
  \end{algorithmic}
\end{algorithm}

The algorithm amazes with its elegance and simplicity.
Computational complexity is dominated by the partial sort
(which can be implemented in $O(n + s \log n)$,
or even in $O(n)$ for floating-point numbers
[@terdiman_radix_2000].
However, generating random variates is computationally expensive,
and for $s \ll n$ this may dominate.
\fxnote{Parallelize}.
To overcome this issue in this case,
@efraimidis_weighted_2006
describe *reservoir sampling with exponential jumps* -- 
an extension where each generated random number decides how many items are skipped
until the current "least likely" item is removed from the reservoir.
Only $O(s \log \frac{n}{s})$ random numbers (in expectation) are needed with this extension,
the simple version always requires $n$ random numbers.
We refer to the original paper for more details, including formal proofs of correctness.


# Code

The \pkg{wrswoR} package contains implementations for the two algorithms
presented in the previous section, one \proglang{R} implementation
of rejection sampling (\cref{alg:sample-rej}, denoted by *rej*),
two implementations (\proglang{R} and \proglang{C++})
of simplified reservoir sampling (\cref{alg:sample-rank}, *rank* and *crank*)
and one \proglang{C++} implementation of reservoir sampling with exponential jumps
(*expj*).
In the package, the corresponding functions are prefixed with \code{sample_int_}.
The \pkg{Rcpp} package
`r knitcitations::citep(citation("Rcpp"))`
is used to generate the glue between \proglang{R} and \proglang{C++}.

The \proglang{R} implementations are very similar to the pseudocode:
As an example, the *rank* implementation is shown below.

```{r}
sample_int_rank
```

The *crank* implementation has been somewhat optimized for cache efficiency.
Due to its relative complexity, the *expj* implementation is kept very close
to the pseudocode in the original paper,
still this function also operates on the logarithmic scale for numeric stability.
The transformation works in a fashion very similar to that of
\cref{alg:sample-rank}.

All functions share the same interface.
Compared to \code{sample.int}, the base \proglang{R} function,
the \code{replace} parameter has been removed, and \code{prob} cannot be \code{NULL}.
To simplify testing the new routines against the \proglang{R} implementation,
a wrapper function \code{sample_int_R} is provided.
The subsequent section shows performance characteristics and correctness
of the new implementations compared to the base \proglang{R} version.


# Tests

This section presents run time tests for various combinations of input parameters,
attempts to provide guidance when to choose which implementation,
and discusses the correctness of the implementation.
All test results shown in this section are based on data available
in the \pkg{wrswoR.benchmark} package.


## Input parameters

The run time tests used different values for the function arguments
\code{n}, \code{size} and \code{prob}.
Instead of directly specifying \code{size}, it is given as a fraction of \code{n}.
The following weight distributions (used for \code{prob}) were tested:

- *Uniform*
- *Linear*: Sequence from \code{1} to \code{n}, *ascending*, *descending* and *shuffled*
- *Geometric*: Starting at \code{1}, the weight is multiplied with a constant for each step
  (*ascending* and *descending*); the constant is chosen so that minimal and maximal weight
  and the sum of weights is still representable as a floating-point number.
  \fxwarning{Extreme case}

```{r echo=FALSE}
ggplot_base <- list(
  theme_bw(11)
)

ALGOS <- c("R", "rej", "rank", "crank", "expj")

REMOVE_PROB <- c("linear_double", "linear_half")

timings <- 
  wrswoR.benchmark::timings %>%
  tbl_df %>%
  filter(expr %in% ALGOS) %>%
  filter(!(prob %in% REMOVE_PROB)) %>%
  mutate(expr = factor(expr, levels = ALGOS)) %>%
  mutate(prob = factor(
    prob, levels = c("uniform", "linear", "rev_linear", "shuffled_linear", "exp", "rexp"),
    labels = c("Uniform", "Linear asc.", "Linear desc.", "Linear shuffled", "Geometric asc.", "Geometric desc.")))

BASE <- 1.007
N <- max(timings$n)
```


```{r echo=FALSE}
probs <- wrswoR.benchmark::prob(1.007)

prob_data <- ldply(
  probs,
  function(f) {
    y <- f(N)
    xi <- round(seq.int(1, N, length.out = 271*3))
    data.frame(x=xi, y=prop.table(y[xi]))
  }
)
```

```{r echo=FALSE}
ggplot_perf_base <-
  ggplot_base %>%
  c(list(
    theme_bw(11),
    scale_color_discrete(name = "Algorithm")
  ))

ggplot_time_base <-
  ggplot_base %>%
  c(list(ylab("Run time (s)")))

ggplot_time_per_item_base <-
  ggplot_base %>%
  c(list(ylab("Run time per element (s)")))
```


## Run time

The run time was measured using the \pkg{microbenchmark} package
`r knitcitations::citep(citation("microbenchmark"))`
in block order with a warmup of 10 iterations
using the default 100 iterations.
The tests ran on a single core of an Intel(R) Xeon(R) CPU E5-2643 clocked at 3.30 GHz
with 10 MB cache.
\fxwarning{Which version of the package?}

\Cref{fig:run-time-log} presents an overview of the median run time
for different input sizes, output size ratios, weight distributions and algorithms.
The \proglang{R} implementation is clearly outperformed by all other implementations
for $n = `r max(timings$n)`$, in many cases even for much smaller inputs.
In the log-log scale used here, the slope of the curves translates to computational complexity;
the steeper slope for the \proglang{R} implementation corresponds to its quadratic complexity
compared to the (in most cases) slightly superlinear complexity of the other algorithms.

As expected, the *expj* algorithm is among the fastest, especially for $r \ll 1$.
In the case $r = 0.01$ for the *geometric ascending* distribution,
the new implementations win only by a margin;
in particular, the *expj* algorithm depends on the ordering of the weights
which is unfavorable here.

The *rej* and *rank* algorithms exhibit initial costs on the sub-millisecond scale
even for small input sizes, probably due to the fact that both are implemented
purely in \proglang{R}.
In addition, the *rej* algorithm is by far the slowest
(but still faster than the stock implementation)
for *geometric* distributions, because in each step only a tiny fraction of items
have a non-negligible weight, and hence most sampled items are rejected as duplicates
(line \ref{alg:sample-rej:sample} of \cref{alg:sample-rej}).
The remainder of the paper will focus on evaluating the *crank* and *expj* implementations.





- Distribution type matters only for expj and $r \ll 1$, and somewhat for crank and rank
    - expj: pre-sorted data
- analyze difference between rank and crank
- focus on crank and expj
- 

\fxwarning{Show probability distributions}


```{r run-time-log, echo=FALSE, fig.height=7, fig.cap="More shit"}
timings %>%
  group_by(n, expr, prob, r) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, color=expr)) +
  scale_y_log10() +
  ggplot_perf_base +
  geom_line() +
  scale_x_log10() +
  facet_grid(prob~r, labeller = label_both)
```

```{r run-time-log-2, echo=FALSE, fig.height=7, fig.cap="More shit 2"}
timings %>%
  group_by(n, expr, prob, r) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, linetype=prob)) +
  scale_y_log10() +
  ggplot_perf_base +
  geom_line() +
  scale_x_log10() +
  facet_grid(expr~r, labeller = label_both)
```

```{r run-time-10-log, echo=FALSE}
timings %>%
  filter(r == 0.1) %>%
  filter(prob == "uniform") %>%
  filter(expr != "ccrank") %>%
  group_by(n, expr) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, color=expr)) +
  scale_y_log10() +
  ggplot_perf_base +
  geom_line() +
  scale_x_log10()
```


```{r run-time-100-log, echo=FALSE}
timings %>%
  filter(r == 0.01) %>%
  filter(prob == "uniform") %>%
  filter(expr != "ccrank") %>%
  group_by(n, expr) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, color=expr)) +
  scale_y_log10() +
  ggplot_perf_base +
  geom_line() +
  scale_x_log10()
```


```{r run-time-100-linear-log, echo=FALSE}
timings %>%
  filter(r == 0.01) %>%
  filter(prob == "shuffled_linear") %>%
  filter(expr != "ccrank") %>%
  group_by(n, expr) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, color=expr)) +
  scale_y_log10() +
  ggplot_perf_base +
  geom_line() +
  scale_x_log10()
```


```{r run-time-10-linear-log, echo=FALSE}
timings %>%
  filter(r == 0.1) %>%
  filter(prob == "linear") %>%
  filter(expr != "ccrank") %>%
  group_by(n, expr) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, color=expr)) +
  scale_y_log10() +
  ggplot_perf_base +
  geom_line() +
  scale_x_log10()
```

```{r run-time-linear-log, echo=FALSE}
timings %>%
  filter(r == 1) %>%
  filter(prob == "linear") %>%
  filter(expr != "ccrank") %>%
  group_by(n, expr) %>%
  summarize(median = median(time)) %>%
  ungroup %>%
  ggplot(aes(x=n, y=median * 1e-9, color=expr)) +
  scale_y_log10() +
  ggplot_perf_base +
  geom_line() +
  scale_x_log10()
```


## Linear model

## Correctness

# Conclusions and future work

- Pre-sort weights using radix sort

# References {.unnumbered}

\justifying

```{r echo=FALSE, message=FALSE, cache=FALSE}
knitcitations::write.bibtex()
```
